import os
from datasets import load_dataset
from huggingface_hub import snapshot_download

# é…ç½®è·¯å¾„ (å¿…é¡»ä¸ä¹‹å‰ä¸€è‡´)
PROJECT_ROOT = os.getcwd()
CACHE_DIR = os.path.join(PROJECT_ROOT, "dataset_cache")

os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
os.environ["HF_HOME"] = CACHE_DIR
os.environ["HF_DATASETS_CACHE"] = CACHE_DIR

# éœ€è¦è¡¥é½çš„ç¼ºå¤±æ•°æ®é›†
MISSING_DATASETS = [
    # å­¦æœ¯çŸ¥è¯†
    {"name": "ARC-Challenge", "path": "allenai/ai2_arc", "config": "ARC-Challenge", "split": "test"},
    # å¸¸è¯†
    {"name": "HellaSwag", "path": "Rowan/hellaswag", "config": None, "split": "validation"},
    # ç‰©ç† (å¯èƒ½éœ€è¦å¿«ç…§ä¸‹è½½)
    {"name": "PIQA", "path": "piqa", "config": None, "split": "validation"},
    # æŒ‡ä»¤éµå¾ª
    {"name": "IFEval", "path": "google/IFEval", "config": None, "split": "train"}
]

def fix_others():
    print("ğŸš€ å¼€å§‹è¡¥é½ ARC, HellaSwag, PIQA, IFEval...")
    print(f"ğŸ“‚ ç›®æ ‡ç¼“å­˜: {CACHE_DIR}\n")
    
    for item in MISSING_DATASETS:
        print(f"æ­£åœ¨ä¸‹è½½: {item['name']} ... ", end="", flush=True)
        try:
            # å°è¯•å¸¸è§„ä¸‹è½½
            load_dataset(item['path'], item['config'], cache_dir=CACHE_DIR, trust_remote_code=True)
            print("âœ… æˆåŠŸ")
        except Exception as e:
            # PIQA ç‰¹æ®Šå¤„ç†
            if "piqa" in item['path']:
                print("\n   âš ï¸  è§¦å‘ PIQA å¿«ç…§ä¿®å¤...", end="")
                try:
                    local_dir = os.path.join(CACHE_DIR, "piqa")
                    snapshot_download(repo_id="piqa", repo_type="dataset", local_dir=local_dir, local_dir_use_symlinks=False)
                    load_dataset(local_dir, cache_dir=CACHE_DIR, trust_remote_code=True)
                    print("âœ… æˆåŠŸ")
                except Exception as e2:
                    print(f"âŒ PIQA æœ€ç»ˆå¤±è´¥: {e2}")
            else:
                print(f"âŒ å¤±è´¥: {str(e)[:100]}")

if __name__ == "__main__":
    fix_others()