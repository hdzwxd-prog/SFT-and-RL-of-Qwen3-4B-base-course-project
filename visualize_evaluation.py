#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Evaluation Results Visualization Script
Analyzes and visualizes evaluation results generated by LLaMA Factory
"""
import os
import json
import argparse
from pathlib import Path
from typing import Dict, List
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Set English font settings
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False
sns.set_style("whitegrid")
plt.style.use('seaborn-v0_8-darkgrid')


def load_evaluation_results(results_dir: str) -> Dict:
    """Load evaluation results"""
    results = {}
    
    # Load metrics
    metrics_file = os.path.join(results_dir, 'eval_results.json')
    if os.path.exists(metrics_file):
        with open(metrics_file, 'r', encoding='utf-8') as f:
            results['metrics'] = json.load(f)
    
    # Load prediction results
    predictions_file = os.path.join(results_dir, 'generated_predictions.jsonl')
    if os.path.exists(predictions_file):
        predictions = []
        with open(predictions_file, 'r', encoding='utf-8') as f:
            for line in f:
                predictions.append(json.loads(line))
        results['predictions'] = predictions
    
    return results


def compute_metrics_from_predictions(predictions: List[Dict]) -> Dict[str, float]:
    """Compute metrics from prediction results"""
    metrics = {}
    
    # Extract predictions and labels
    preds = [p.get('predict', '') for p in predictions]
    labels = [p.get('label', '') for p in predictions]
    
    # BLEU scores
    try:
        from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
        smoothing = SmoothingFunction().method3
        bleu_scores = []
        for pred, ref in zip(preds, labels):
            if ref.strip():
                score = sentence_bleu([ref.split()], pred.split(), smoothing_function=smoothing)
                bleu_scores.append(score)
        metrics['bleu'] = np.mean(bleu_scores) if bleu_scores else 0.0
    except ImportError:
        metrics['bleu'] = 0.0
    
    # ROUGE scores
    try:
        from rouge_score import rouge_scorer
        scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)
        rouge_scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}
        for pred, ref in zip(preds, labels):
            scores = scorer.score(ref, pred)
            rouge_scores['rouge1'].append(scores['rouge1'].fmeasure)
            rouge_scores['rouge2'].append(scores['rouge2'].fmeasure)
            rouge_scores['rougeL'].append(scores['rougeL'].fmeasure)
        metrics['rouge1'] = np.mean(rouge_scores['rouge1'])
        metrics['rouge2'] = np.mean(rouge_scores['rouge2'])
        metrics['rougeL'] = np.mean(rouge_scores['rougeL'])
    except ImportError:
        metrics['rouge1'] = metrics['rouge2'] = metrics['rougeL'] = 0.0
    
    # Length statistics
    pred_lengths = [len(p.split()) for p in preds]
    ref_lengths = [len(r.split()) for r in labels]
    metrics['avg_pred_length'] = np.mean(pred_lengths)
    metrics['avg_ref_length'] = np.mean(ref_lengths)
    
    return metrics, preds, labels


def create_comprehensive_metrics(results: Dict, output_dir: str):
    """Create comprehensive metrics visualization with radar chart, bar chart, and summary table"""
    os.makedirs(output_dir, exist_ok=True)
    
    # Calculate metrics if predictions are available
    if 'predictions' in results:
        metrics, preds, labels = compute_metrics_from_predictions(results['predictions'])
    elif 'metrics' in results:
        metrics = results['metrics']
        preds = labels = []
    else:
        print("Warning: No evaluation results found")
        return
    
    # Create figure with subplots
    fig = plt.figure(figsize=(16, 12))
    
    # 1. Radar chart (top left) - shows metrics distribution
    ax1 = plt.subplot(2, 2, 1, projection='polar')
    radar_metrics = {
        'BLEU': metrics.get('bleu', 0),
        'ROUGE-1': metrics.get('rouge1', 0),
        'ROUGE-2': metrics.get('rouge2', 0),
        'ROUGE-L': metrics.get('rougeL', 0),
    }
    
    categories = list(radar_metrics.keys())
    values = list(radar_metrics.values())
    N = len(categories)
    angles = np.linspace(0, 2 * np.pi, N, endpoint=False).tolist()
    values += values[:1]
    angles += angles[:1]
    
    ax1.plot(angles, values, 'o-', linewidth=2, color='#3498db')
    ax1.fill(angles, values, alpha=0.25, color='#3498db')
    ax1.set_xticks(angles[:-1])
    ax1.set_xticklabels(categories, size=11)
    ax1.set_ylim(0, 1.0)
    ax1.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])
    ax1.set_yticklabels(['0.2', '0.4', '0.6', '0.8', '1.0'], size=9)
    ax1.grid(True)
    ax1.set_title('Metrics Radar Chart', size=14, fontweight='bold', pad=20)
    
    # 2. Bar chart for BLEU and ROUGE (top right) - shows metric scores
    ax2 = plt.subplot(2, 2, 2)
    metric_names = ['BLEU', 'ROUGE-1', 'ROUGE-2', 'ROUGE-L']
    metric_values = [
        metrics.get('bleu', 0) * 100,
        metrics.get('rouge1', 0) * 100,
        metrics.get('rouge2', 0) * 100,
        metrics.get('rougeL', 0) * 100
    ]
    bars = ax2.bar(metric_names, metric_values, color=['#3498db', '#2ecc71', '#e74c3c', '#f39c12'])
    ax2.set_ylabel('Score (%)', fontsize=12)
    ax2.set_title('BLEU and ROUGE Scores', fontsize=14, fontweight='bold')
    ax2.set_ylim([0, 100])
    ax2.grid(axis='y', alpha=0.3)
    for bar in bars:
        height = bar.get_height()
        ax2.text(bar.get_x() + bar.get_width()/2., height,
                f'{height:.2f}%', ha='center', va='bottom', fontsize=10)
    
    # 3. Summary table (bottom) - displays all metrics in tabular format
    ax3 = plt.subplot(2, 1, 2)
    ax3.axis('off')
    table_data = [
        ['Metric', 'Value'],
        ['BLEU Score', f"{metrics.get('bleu', 0)*100:.2f}%"],
        ['ROUGE-1 F1', f"{metrics.get('rouge1', 0)*100:.2f}%"],
        ['ROUGE-2 F1', f"{metrics.get('rouge2', 0)*100:.2f}%"],
        ['ROUGE-L F1', f"{metrics.get('rougeL', 0)*100:.2f}%"],
    ]
    if 'avg_pred_length' in metrics:
        table_data.append(['Avg Predicted Length', f"{metrics['avg_pred_length']:.1f} words"])
        table_data.append(['Avg Reference Length', f"{metrics['avg_ref_length']:.1f} words"])
        if metrics.get('avg_ref_length', 0) > 0:
            length_ratio = metrics.get('avg_pred_length', 0) / metrics.get('avg_ref_length', 1)
            table_data.append(['Length Ratio', f"{length_ratio:.2f}"])
    
    table = ax3.table(cellText=table_data, cellLoc='left', loc='center', colWidths=[0.4, 0.3])
    table.auto_set_font_size(False)
    table.set_fontsize(12)
    table.scale(1.2, 2.5)
    for i in range(len(table_data[0])):
        table[(0, i)].set_facecolor('#3498db')
        table[(0, i)].set_text_props(weight='bold', color='white')
    
    plt.suptitle('Comprehensive Evaluation Metrics', fontsize=16, fontweight='bold', y=0.98)
    plt.tight_layout(rect=[0, 0, 1, 0.97])
    plt.savefig(os.path.join(output_dir, 'comprehensive_metrics.png'), dpi=300, bbox_inches='tight')
    print(f"Comprehensive metrics saved: {output_dir}/comprehensive_metrics.png")
    plt.close()


def create_predictions_analysis(results: Dict, output_dir: str):
    """Create predictions analysis visualization with length distributions and scatter plots"""
    os.makedirs(output_dir, exist_ok=True)
    
    # Calculate metrics if predictions are available
    if 'predictions' in results:
        metrics, preds, labels = compute_metrics_from_predictions(results['predictions'])
    elif 'metrics' in results:
        metrics = results['metrics']
        preds = labels = []
    else:
        print("Warning: No prediction data found for analysis")
        return
    
    if not preds or not labels:
        print("Warning: No prediction data available for analysis")
        return
    
    # Create figure with subplots
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    
    pred_lengths = [len(p.split()) for p in preds]
    ref_lengths = [len(r.split()) for r in labels]
    
    # 1. Length distribution histogram (top left) - compares predicted vs reference lengths
    ax1 = axes[0, 0]
    ax1.hist([pred_lengths, ref_lengths], bins=30, alpha=0.7,
             label=['Predicted', 'Reference'], color=['#3498db', '#e74c3c'])
    ax1.set_xlabel('Length (words)', fontsize=12)
    ax1.set_ylabel('Frequency', fontsize=12)
    ax1.set_title('Response Length Distribution', fontsize=14, fontweight='bold')
    ax1.legend()
    ax1.grid(axis='y', alpha=0.3)
    
    # 2. Length ratio distribution (top right) - shows how predicted length relates to reference
    ax2 = axes[0, 1]
    length_ratios = [len(p.split())/len(r.split()) if len(r.split()) > 0 else 0
                     for p, r in zip(preds, labels)]
    ax2.hist(length_ratios, bins=30, color='#9b59b6', alpha=0.7)
    ax2.axvline(x=1.0, color='red', linestyle='--', linewidth=2, label='Ideal Ratio (1.0)')
    ax2.set_xlabel('Length Ratio (Predicted / Reference)', fontsize=12)
    ax2.set_ylabel('Frequency', fontsize=12)
    ax2.set_title('Length Ratio Distribution', fontsize=14, fontweight='bold')
    ax2.legend()
    ax2.grid(axis='y', alpha=0.3)
    
    # 3. Scatter plot (bottom left) - shows correlation between predicted and reference lengths
    ax3 = axes[1, 0]
    ax3.scatter(ref_lengths, pred_lengths, alpha=0.5, color='#2ecc71', s=50)
    # Add diagonal line for ideal case
    max_len = max(max(ref_lengths), max(pred_lengths)) if ref_lengths and pred_lengths else 500
    ax3.plot([0, max_len], [0, max_len], 'r--', linewidth=2, label='Ideal (y=x)')
    ax3.set_xlabel('Reference Length (words)', fontsize=12)
    ax3.set_ylabel('Predicted Length (words)', fontsize=12)
    ax3.set_title('Predicted vs Reference Length', fontsize=14, fontweight='bold')
    ax3.legend()
    ax3.grid(alpha=0.3)
    
    # 4. Length statistics summary (bottom right) - displays numerical statistics
    ax4 = axes[1, 1]
    ax4.axis('off')
    stats_data = [
        ['Statistic', 'Value'],
        ['Total Samples', f"{len(preds)}"],
        ['Avg Predicted Length', f"{np.mean(pred_lengths):.1f} words"],
        ['Avg Reference Length', f"{np.mean(ref_lengths):.1f} words"],
        ['Median Predicted Length', f"{np.median(pred_lengths):.1f} words"],
        ['Median Reference Length', f"{np.median(ref_lengths):.1f} words"],
        ['Std Predicted Length', f"{np.std(pred_lengths):.1f} words"],
        ['Std Reference Length', f"{np.std(ref_lengths):.1f} words"],
    ]
    if np.mean(ref_lengths) > 0:
        length_ratio = np.mean(pred_lengths) / np.mean(ref_lengths)
        stats_data.append(['Avg Length Ratio', f"{length_ratio:.2f}"])
    
    stats_table = ax4.table(cellText=stats_data, cellLoc='left', loc='center', colWidths=[0.5, 0.3])
    stats_table.auto_set_font_size(False)
    stats_table.set_fontsize(11)
    stats_table.scale(1.2, 2)
    for i in range(len(stats_data[0])):
        stats_table[(0, i)].set_facecolor('#2ecc71')
        stats_table[(0, i)].set_text_props(weight='bold', color='white')
    
    plt.suptitle('Predictions Analysis', fontsize=16, fontweight='bold')
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'predictions_analysis.png'), dpi=300, bbox_inches='tight')
    print(f"Predictions analysis saved: {output_dir}/predictions_analysis.png")
    plt.close()


def create_visualizations(results: Dict, output_dir: str):
    """Create visualization charts"""
    os.makedirs(output_dir, exist_ok=True)
    
    # Calculate metrics if predictions are available
    if 'predictions' in results:
        metrics, preds, labels = compute_metrics_from_predictions(results['predictions'])
    elif 'metrics' in results:
        metrics = results['metrics']
        preds = labels = []
    else:
        print("Warning: No evaluation results found")
        return
    
    # 1. Basic metrics visualization
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    fig.suptitle('Model Evaluation Metrics', fontsize=16, fontweight='bold')
    
    # BLEU and ROUGE
    ax1 = axes[0, 0]
    metric_names = ['BLEU', 'ROUGE-1', 'ROUGE-2', 'ROUGE-L']
    metric_values = [
        metrics.get('bleu', 0) * 100,
        metrics.get('rouge1', 0) * 100,
        metrics.get('rouge2', 0) * 100,
        metrics.get('rougeL', 0) * 100
    ]
    bars = ax1.bar(metric_names, metric_values, color=['#3498db', '#2ecc71', '#e74c3c', '#f39c12'])
    ax1.set_ylabel('Score (%)', fontsize=12)
    ax1.set_title('BLEU and ROUGE Scores', fontsize=14, fontweight='bold')
    ax1.set_ylim([0, 100])
    ax1.grid(axis='y', alpha=0.3)
    for bar in bars:
        height = bar.get_height()
        ax1.text(bar.get_x() + bar.get_width()/2., height,
                f'{height:.2f}%', ha='center', va='bottom', fontsize=10)
    
    # Length distribution (if data available)
    if preds and labels:
        ax2 = axes[0, 1]
        pred_lengths = [len(p.split()) for p in preds]
        ref_lengths = [len(r.split()) for r in labels]
        ax2.hist([pred_lengths, ref_lengths], bins=30, alpha=0.7,
                 label=['Predicted', 'Reference'], color=['#3498db', '#e74c3c'])
        ax2.set_xlabel('Length (words)', fontsize=12)
        ax2.set_ylabel('Frequency', fontsize=12)
        ax2.set_title('Response Length Distribution', fontsize=14, fontweight='bold')
        ax2.legend()
        ax2.grid(axis='y', alpha=0.3)
        
        # Length ratio
        ax3 = axes[1, 0]
        length_ratios = [len(p.split())/len(r.split()) if len(r.split()) > 0 else 0
                         for p, r in zip(preds, labels)]
        ax3.hist(length_ratios, bins=30, color='#9b59b6', alpha=0.7)
        ax3.axvline(x=1.0, color='red', linestyle='--', linewidth=2, label='Ideal Ratio (1.0)')
        ax3.set_xlabel('Length Ratio (Predicted / Reference)', fontsize=12)
        ax3.set_ylabel('Frequency', fontsize=12)
        ax3.set_title('Length Ratio Distribution', fontsize=14, fontweight='bold')
        ax3.legend()
        ax3.grid(axis='y', alpha=0.3)
    
    # Metrics summary table
    ax4 = axes[1, 1]
    ax4.axis('off')
    table_data = [
        ['Metric', 'Value'],
        ['BLEU', f"{metrics.get('bleu', 0)*100:.2f}%"],
        ['ROUGE-1', f"{metrics.get('rouge1', 0)*100:.2f}%"],
        ['ROUGE-2', f"{metrics.get('rouge2', 0)*100:.2f}%"],
        ['ROUGE-L', f"{metrics.get('rougeL', 0)*100:.2f}%"],
    ]
    if 'avg_pred_length' in metrics:
        table_data.append(['Avg Pred Length', f"{metrics['avg_pred_length']:.1f} words"])
        table_data.append(['Avg Ref Length', f"{metrics['avg_ref_length']:.1f} words"])
    
    table = ax4.table(cellText=table_data, cellLoc='left', loc='center', colWidths=[0.6, 0.4])
    table.auto_set_font_size(False)
    table.set_fontsize(12)
    table.scale(1.2, 2)
    for i in range(len(table_data[0])):
        table[(0, i)].set_facecolor('#3498db')
        table[(0, i)].set_text_props(weight='bold', color='white')
    ax4.set_title('Metrics Summary', fontsize=14, fontweight='bold', pad=20)
    
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'evaluation_metrics.png'), dpi=300, bbox_inches='tight')
    print(f"Visualization saved: {output_dir}/evaluation_metrics.png")
    plt.close()
    
    # Create comprehensive metrics and predictions analysis
    create_comprehensive_metrics(results, output_dir)
    create_predictions_analysis(results, output_dir)


def save_examples(predictions: List[Dict], output_dir: str, num_examples: int = 20):
    """Save prediction examples"""
    os.makedirs(output_dir, exist_ok=True)
    
    examples = []
    for i, pred in enumerate(predictions[:num_examples]):
        examples.append({
            'index': i + 1,
            'prompt': pred.get('prompt', ''),
            'reference': pred.get('label', ''),
            'prediction': pred.get('predict', ''),
        })
    
    # Save JSON
    with open(os.path.join(output_dir, 'examples.json'), 'w', encoding='utf-8') as f:
        json.dump(examples, f, ensure_ascii=False, indent=2)
    
    # Save Markdown
    md_content = "# Model Evaluation Examples\n\n"
    for ex in examples:
        md_content += f"## Example {ex['index']}\n\n"
        md_content += f"### Prompt:\n```\n{ex['prompt']}\n```\n\n"
        md_content += f"### Reference:\n```\n{ex['reference']}\n```\n\n"
        md_content += f"### Prediction:\n```\n{ex['prediction']}\n```\n\n---\n\n"
    
    with open(os.path.join(output_dir, 'examples.md'), 'w', encoding='utf-8') as f:
        f.write(md_content)
    
    print(f"Examples saved: {output_dir}/examples.json and {output_dir}/examples.md")


def main():
    parser = argparse.ArgumentParser(description='Visualize evaluation results')
    parser.add_argument('--results_dir', type=str, required=True,
                       help='Evaluation results directory (containing eval_results.json or generated_predictions.jsonl)')
    parser.add_argument('--output_dir', type=str, default=None,
                       help='Output directory for visualizations (default: same as results_dir)')
    parser.add_argument('--num_examples', type=int, default=20,
                       help='Number of examples to save')
    
    args = parser.parse_args()
    
    output_dir = args.output_dir or args.results_dir
    
    print("=" * 60)
    print("Evaluation Results Visualization")
    print("=" * 60)
    print(f"Results directory: {args.results_dir}")
    print(f"Output directory: {output_dir}")
    print("=" * 60)
    
    # Load results
    results = load_evaluation_results(args.results_dir)
    
    if not results:
        print("Warning: No evaluation result files found")
        return
    
    # Create visualizations
    create_visualizations(results, output_dir)
    
    # Save examples
    if 'predictions' in results:
        save_examples(results['predictions'], output_dir, args.num_examples)
    
    print(f"\nVisualization completed! Results saved to: {output_dir}")


if __name__ == '__main__':
    main()

