### ========================================
### 模型配置 (Model Configuration)
### ========================================

# 模型路径或HuggingFace模型ID
# 设置为本地路径 ./Qwen3-4B-Base（下载后的模型目录）
# 理由: 使用本地模型可以避免训练时的网络依赖，提高稳定性
# 如果使用HuggingFace模型ID，可改为: Qwen/Qwen3-4B-Base
model_name_or_path: ./Qwen3-4B-Base

# 信任远程代码执行
# 理由: Qwen模型需要执行自定义代码来正确加载，设置为true以允许执行模型仓库中的自定义代码
trust_remote_code: true


### ========================================
### 训练方法配置 (Training Method Configuration)
### ========================================

# 训练阶段：sft (Supervised Fine-Tuning, 有监督微调)
# 理由: 使用指令跟随数据集进行有监督微调，这是LLM微调的标准阶段
stage: sft

# 是否执行训练
# 理由: 设置为true以启动训练流程
do_train: true

# 微调类型：lora (Low-Rank Adaptation)
# 理由: LoRA是一种参数高效的微调方法，只训练少量参数（约0.1%-1%），显存占用小，训练速度快
# 相比全量微调，LoRA可以大幅降低显存需求，适合在2张A6000上训练4B模型
finetuning_type: lora

# LoRA应用目标：all (所有线性层)
# 理由: 对所有线性层（attention和MLP）应用LoRA，可以获得更好的微调效果
# 其他选项: q_proj, k_proj, v_proj, o_proj (仅attention层)
lora_target: all

# LoRA秩 (rank)：16
# 理由: rank控制LoRA矩阵的秩，影响可训练参数量和表达能力
# rank=16 是4B模型的常见选择，在效果和效率之间取得平衡
# 更小的rank (8) 显存更省但效果可能略差，更大的rank (32) 效果更好但显存占用更大
# 对于A6000的48GB显存，rank=16是安全且有效的选择
lora_rank: 16

# LoRA缩放因子 (alpha)：32
# 理由: alpha控制LoRA权重的缩放，通常设为rank的2倍
# alpha/rank的比例影响LoRA对原始权重的影响程度，2:1是经验上的最佳比例
# 如果rank=16，alpha=32 意味着LoRA的影响程度适中
lora_alpha: 32

# LoRA Dropout率：0.05
# 理由: 轻微的dropout可以防止过拟合，提高模型泛化能力
# 0.05是较小的dropout值，适合在已有预训练权重基础上的微调
# 如果数据集较小或出现过拟合，可以适当增加到0.1
lora_dropout: 0.05


### ========================================
### 数据集配置 (Dataset Configuration)
### ========================================

# 数据集名称：在 data/dataset_info.json 中定义的数据集名称
# 理由: 使用本地数据集配置，便于管理和复用
# 注意: 必须与 data/dataset_info.json 中定义的名称一致（当前为 tulu3_sft_personas）
# 如果直接使用HuggingFace数据集，需要在 dataset_info.json 中添加 hf_hub_url 配置
dataset: tulu3_sft_personas

# 模板类型：qwen3
# 理由: 必须与模型匹配，Qwen3-4B-Base需要使用qwen3模板来正确格式化输入
# 模板定义了系统提示词、用户提示词、助手回复的格式，错误的模板会导致训练失败
template: qwen3

# 最大序列长度：8192 tokens
# 理由: 限制每个训练样本的最大长度，超出部分会被截断
# 8192对于指令微调任务来说是合理的长度，既能处理长对话，又不会占用过多显存
# Qwen3原生支持32k，但考虑到2张A6000的显存限制，8192是平衡性能和资源的选择
# 如果显存不足，可以减小到4096或2048
cutoff_len: 8192

# 是否覆盖已存在的缓存
# 理由: 设置为true确保数据预处理时重新生成缓存，避免使用旧的缓存导致错误
# 如果数据没有变化，可以改为false以加速启动
overwrite_cache: true

# 数据预处理工作进程数：16
# 理由: 并行处理数据可以加速数据加载，16是4B模型训练的常用值
# 可以根据CPU核心数调整，通常设为CPU核心数的一半到全部
preprocessing_num_workers: 16


### ========================================
### 输出配置 (Output Configuration)
### ========================================

# 输出目录：保存训练检查点和日志
# 理由: 结构化保存路径，便于管理和区分不同实验
output_dir: ./saves/qwen3-4b-base/lora/sft

# 日志记录频率：每10步记录一次
# 理由: 频繁记录日志可以更好地监控训练过程，及时发现问题
# 10步是一个合理的频率，不会产生过多日志文件，又能及时反映训练状态
logging_steps: 10

# 模型保存频率：每500步保存一次检查点
# 理由: 定期保存检查点可以防止训练中断导致进度丢失
# 500步的保存频率在训练速度和安全性之间取得平衡
# 如果训练步数较少，可以适当减小；如果训练步数很多，可以适当增大
save_steps: 500

# 是否绘制损失曲线
# 理由: 可视化损失曲线有助于观察训练趋势，判断是否收敛或过拟合
plot_loss: true

# 是否覆盖输出目录
# 理由: 设置为true时，如果输出目录已存在会覆盖，避免旧文件干扰
# 如果不想覆盖，可以改为false，系统会自动创建带时间戳的新目录
overwrite_output_dir: true


### ========================================
### 训练超参数配置 (Training Hyperparameters)
### ========================================

# 每个设备的训练批次大小：2
# 理由: 单GPU批次大小，考虑到A6000的48GB显存和8192的序列长度
# batch_size=2 可以在显存和训练稳定性之间取得平衡
# 如果显存充足，可以增加到4；如果显存不足，可以减小到1
# 有效批次大小 = per_device_train_batch_size × gradient_accumulation_steps × GPU数量
# 当前配置: 2 × 8 × 2 = 32（对于4B模型来说是合理的批次大小）
per_device_train_batch_size: 2

# 梯度累积步数：8
# 理由: 通过累积梯度来模拟更大的批次大小，同时不增加显存占用
# 累积8步相当于将有效批次大小增加到32（2 GPUs × 2 × 8）
# 较大的批次大小有助于训练稳定性，提高模型收敛质量
# 如果显存不足，可以增大此值并减小per_device_train_batch_size
gradient_accumulation_steps: 8

# 学习率：2.0e-4 (0.0002)
# 理由: LoRA微调通常使用较大的学习率（相比全量微调），因为只更新少量参数
# 2.0e-4 是LoRA训练的常用学习率，对于4B模型来说是一个安全且有效的起始值
# 如果训练不稳定（损失震荡），可以减小到1.0e-4；如果收敛太慢，可以适当增大到3.0e-4
learning_rate: 2.0e-4

# 训练轮数：3.0
# 理由: 对于指令微调任务，3个epoch通常是足够的
# 过多的epoch可能导致过拟合，特别是当数据集不是特别大时
# 可以根据验证集表现调整，如果验证损失还在下降，可以适当增加
num_train_epochs: 3.0

# 学习率调度器类型：cosine (余弦退火)
# 理由: 余弦退火调度器可以平滑地降低学习率，有助于模型在训练后期精细调整
# 相比线性衰减，余弦退火通常能获得更好的最终性能
# 其他选项: linear (线性), constant (恒定), constant_with_warmup (带预热的恒定)
lr_scheduler_type: cosine

# 预热比例：0.1 (前10%的训练步数用于预热)
# 理由: 学习率预热可以让模型在训练初期逐步适应，避免突然的大梯度导致训练不稳定
# 10%的预热比例是经验上的合理值，可以让模型平稳过渡到正常学习率
# 如果训练不稳定，可以适当增加预热比例到0.15或0.2
warmup_ratio: 0.1

# 是否使用bfloat16混合精度训练
# 理由: bf16可以在保持训练稳定性的同时大幅降低显存占用和加速训练
# 相比fp16，bf16的数值范围更大，不容易出现梯度下溢或上溢
# A6000支持bf16，使用bf16可以节省约50%的显存，这是必选项
# 如果GPU不支持bf16，可以改为fp16: true
bf16: true

# 是否启用梯度检查点 (Gradient Checkpointing)
# 理由: 梯度检查点通过牺牲计算时间来节省显存，可以将前向传播的中间激活值不全部保存
# 对于长序列（8192）和有限显存（48GB），启用梯度检查点是必要的
# 大约可以节省30-40%的显存，虽然会增加约20%的训练时间，但可以支持更大的batch size或序列长度
gradient_checkpointing: true

# DDP超时时间：180000000秒（约5年）
# 理由: 多GPU训练时的同步超时时间，设置为很大的值以避免长时间训练时的超时问题
# 对于分布式训练，如果某个GPU响应慢，超过此时间会报错
# 设置为很大的值（实际不会达到）可以避免不必要的超时错误
ddp_timeout: 1800


### ========================================
### 评估配置 (Evaluation Configuration)
### ========================================

# 验证集比例：0.1 (10%)
# 理由: 从训练集中划分10%作为验证集，用于评估模型性能和早停
# 10%是一个常见的比例，既保证有足够的验证数据，又不浪费训练数据
# 如果数据集很大，可以适当减小；如果数据集很小，可以适当增大
val_size: 0.1

# 每个设备的评估批次大小：2
# 理由: 评估时不需要梯度计算，可以使用与训练相同的batch size或稍大一些
# 设置为2与训练一致，保持配置的统一性
per_device_eval_batch_size: 2

# 评估策略：steps (按步数评估)
# 理由: 按步数进行评估，可以更灵活地控制评估频率
# 其他选项: epoch (按轮数评估), no (不评估)
eval_strategy: steps

# 评估频率：每500步评估一次
# 理由: 与保存检查点的频率一致，每次保存时都进行评估
# 这样可以及时了解模型在验证集上的表现，判断是否需要调整超参数或早停
eval_steps: 50
